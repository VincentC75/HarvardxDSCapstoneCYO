---
title: "HarvardX Data Science Capstone Project"
author: "Vincent"
date: "May 2019"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction


This project is based on the "Rain in Australia" datasaset available on kaggle: <https://www.kaggle.com/jsphyg/weather-dataset-rattle-package>  

.![Weather in Australia](australiaweather.jpg).

The goal is to predict wether or not it will rain tomorrow by training a binary classification model.  

# Preparing data

## Downloading and loading data

The base datafile is downloaded from Kaggle if not already available.

```{r}
# Download base dataset
DataURL <- "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/downloads/
weather-dataset-rattle-package.zip/2"
DataFile <- "weather-dataset-rattle-package.zip"
if (!file.exists(DataFile)) {
  download.file(DataURL, destfile = DataFile)
  unzip(DataFile)
}
```

```{r}
rain <- read.csv("weatherAUS.csv")
str(rain)
```


## Understanding and cleaning data

### Available columns

```{r, include = FALSE}
description <- c('The date of observation', 'The common name of the location of the weather station', 'The minimum temperature in degrees celsius', 'The maximum temperature in degrees celsius', 'The amount of rainfall recorded for the day in mm', 'The so-called Class A pan evaporation (mm) in the 24 hours to 9am', 'The number of hours of bright sunshine in the day', 'The direction of the strongest wind gust in the 24 hours to midnight', 'The speed (km/h) of the strongest wind gust in the 24 hours to midnight', 'Direction of the wind at 9am', 'Direction of the wind at 3pm', 'Wind speed (km/hr) averaged over 10 minutes prior to 9am', 'Wind speed (km/hr) averaged over 10 minutes prior to 3pm', 'Humidity (percent) at 9am', 'Humidity (percent) at 3pm', 'Atmospheric pressure (hpa) reduced to mean sea level at 9am', 'Atmospheric pressure (hpa) reduced to mean sea level at 3pm','Fraction of sky obscured by cloud at 9am. This is measured in "oktas".', 'Fraction of sky obscured by cloud (in "oktas": eighths) at 3pm.', 'Temperature (degrees C) at 9am', 'Temperature (degrees C) at 3pm', 'Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0', 'The amount of next day rain in mm. Used to create response variable RainTomorrow.', 'The target variable. Did it rain tomorrow?')
```

```{r, results='asis', echo=FALSE}
knitr::kable(data.frame(column = names(rain), description))
```

The outcome variable, RainTomorrow, is a factor variable with two levels. This is a binary classification problem

### Date

Date is stored as a factor variable and can be converted to an actual date.

```{r}
rain$Date <- as.Date(rain$Date)
summary(rain$Date)
```

However extracting the month CAN give can be a better predictor because it will have the same value for the same season over the years.

```{r, message = FALSE}
library(lubridate)
rain$Month <- month(ymd(rain$Date))
```


### RISK_MM and RainTomorrow

This predictor is directly linked to the outcome : "The amount of next day rain in mm. Used to create response variable RainTomorrow.". And in fact there is a perfect corelation between those variables. RainTomorrow is True if RISK_MM > 1.

```{r}
table(RainTomorrow = rain$RainTomorrow, RISK_MM_above_1 = rain$RISK_MM > 1)
```
This predictor must be removed because otherwise it would leak the outcome variable in the dataset.

```{r}
rain$RISK_MM <- NULL
```


### Rainfall and RainToday

RainToday is also a binary representation of the numeric variable Rainfall. Raintoday is true when rainfall > 1mm.

```{r}
table(RainToday = rain$RainToday, rainfall_above_1 = rain$Rainfall > 1)
```
Since those variables are prefectly correlated, we will keep only one of them. Rainfall contains more variability so may have a better predictive value.

```{r}
rain$RainToday <- NULL
```

### Missing values

Some variables contain a high percentage of missing values. 

```{r}
sapply(rain, function(x) round(sum(is.na(x)) / length(x) * 100))
```

for some of them, this percentage is so high that trying to impute those values is useless, therefore we keep only those with up to 10% of missing values and eliminate the others: Evaporation, Sunshine, Cloud9am, Cloud3pm

```{r}
rain$Evaporation <- NULL
rain$Sunshine <- NULL
rain$Cloud9am <- NULL
rain$Cloud3pm <- NULL
```

For the remaining data, we could replace NA with the mean for numerical variable or with the mode for factor variables, but since we are dealing with weather data and we have a date, it's better to assign the last non missing values for the same location.

```{r, message = FALSE}
library(dplyr)
library(tidyr)
rain <- rain %>%
  arrange(Date) %>%
  group_by(Location) %>%
  fill(WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Rainfall, Pressure9am,
       Pressure3pm, MinTemp, MaxTemp, Temp9am, Temp3pm, WindGustDir, WindDir9am, WindDir3pm) %>%
  fill(WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Rainfall, Pressure9am,
       Pressure3pm, MinTemp, MaxTemp, Temp9am, Temp3pm, WindGustDir, WindDir9am, WindDir3pm,
       .direction = "up")
rain <- rain %>% ungroup()
rain$Date <- NULL
sapply(rain, function(x) sum(is.na(x)))
```

After this imputation, some values are still missing because certain variables where not collected in some locations, and when all values are missing, we cannot impute the previous value.
We can deal with those special cases.
For WindGustDir we impute the value of WindDir9am for the same observation
For WindGustSpeed, we impute the maximum of WindSpeed9am and WindSpeed3pm for the same observation
For Pressure9am and Pressure3pm we have no value in the same observation thta would make sense, so we impute the mean of these variables in the entire dataset.

```{r}
rain$WindGustDir[is.na(rain$WindGustDir)] <- rain$WindDir9am[is.na(rain$WindGustDir)] 
rain$WindGustSpeed[is.na(rain$WindGustSpeed)] <- max(rain$WindSpeed9am[is.na(rain$WindGustSpeed)],
                                                     rain$WindSpeed3pm[is.na(rain$WindGustSpeed)]) 
rain$Pressure9am[is.na(rain$Pressure9am)] <- mean(rain$Pressure9am[!is.na(rain$Pressure9am)]) 
rain$Pressure3pm[is.na(rain$Pressure3pm)] <- mean(rain$Pressure3pm[!is.na(rain$Pressure3pm)]) 
```

At this stage, we no longer have missing values in the dataset, and we still have 142193 observations. If we had simply ignored all observations with missing values, we would only have 56420 left. This simple imputation allows us to consider almost three times more observations and thus keep more signal in the dataset, hopefully leading to more insight.

# Exploratory Data Analysis

## Prevalence

Let's check the prevalence of the outcome variable
```{r}
prop.table(table(rain$RainTomorrow))
```

We have to take prevalence into account because in 77.5% of cases, the outcome is NO (no rain). So a dummy model always predicting no would be close to 77.5% accurate. We have also to consider sensitivity and specificity. Sensitivity is the ability to predict a positive outcome when the actual outcome is positive. Specificity is the ability not to predict a positive outcome when the actual outcome is not positive.

## Correlation of explanatory variables

The explanotory variables contains different measures of the same quantity. For example we have 4 measures of temperature: min, max, at 9am and at 6pm. As can be expected, there is a relatively strong correlation between these measures.

```{r Temperature Correlation, message = FALSE, warning= FALSE, echo = FALSE, out.height='8cm'}
library(dplyr)
library(corrplot)
corrplot(cor(select(rain, contains("Temp"))), method = "number")
```

```{r Temperature pairs, dev='png', echo = FALSE, out.height='8cm', cache=TRUE}
#pairs(select(rain, contains("Temp")))
```

This is also true for the two measures of pressure.

```{r, dev='png', out.height='8cm', cache=TRUE}
pairs(select(rain, contains("Pressure")))
```

We can outline the highest correlations among the available predictors.

```{r high correlations, out.height='8cm'}
numeric <- sapply(rain, is.numeric)
correlations <- cor(rain[,numeric])
diag(correlations) <- 0
high <- apply(abs(correlations) >= 0.7, 2, any)
corrplot(correlations[high, high], method = "number")
```

Some predictors are indeed highly correlated. For instance, the most obvious corelations are between Temp9am and MinTemp, Temp 3pm and MaxTemp. Pressure9am and Pressure3pm also seem to bring the same information whith a near perfect corelation of 0.96.
Later, in models that are sensitive to corelated predictors, we could choose to keep only MinTemp and MaxTemp and only one measure of Pressure. We would keep most of the signal with 3 predictors instead of 6. 

## Humidity

```{r, message=FALSE, out.width='8cm', out.height='8cm', fig.show='hold'}
library(ggplot2)
rain %>% 
  ggplot(aes(x=RainTomorrow, y=Humidity9am, colour = RainTomorrow, fill= RainTomorrow)) + geom_violin()
rain %>% 
  ggplot(aes(x=RainTomorrow, y=Humidity3pm, colour = RainTomorrow, fill= RainTomorrow)) + geom_violin()
```

We notice that, even if in both cases we are measuring Humidity, there difference is greater in the Humidity3pm measure. It could be interesting to keep only Humidity3pm and eliminate Humidity9am with has a substantial corelation of `round(cor(rain$Humidity9am, rain$Humidity3pm), digits = 2`.


# Models

## Training and testing set
```{r, warning = FALSE, message=FALSE}
library(caret)
set.seed(1971)
test_index <- createDataPartition(rain$RainTomorrow, times = 1, p = 0.2, list = FALSE)
test_set <- rain[test_index, ]
train_set <- rain[-test_index, ]
```

## Baseline model

A simple and naÃ¯ve model would be to always predict the most frequent outcome. It allows us to have a baseline to evaluate more elaborate models.

```{r}
pred_naive <- rep("No", nrow(test_set))
```

### Overall Accuracy
```{r}
mean(pred_naive == test_set$RainTomorrow)
```
As expected, the overall accuracy is close to the prevalence of the most common outcome. However, if we compute the accuracy by outcome, the weakness of our simple approach is revealed. Specificity is perfect but there is no sensitivity.

```{r, message = FALSE}
library(dplyr)
test_set %>%
  mutate(y_hat = pred_naive) %>%
  group_by(RainTomorrow) %>%
  summarize(accuracy = mean(y_hat == RainTomorrow))
```


### Confusion Matrix

```{r}
pred_naive <- as.factor(pred_naive)
levels(pred_naive) <- levels(test_set$RainTomorrow)
table(predicted = pred_naive, actual = test_set$RainTomorrow)
```

```{r}
cm <- confusionMatrix(data = pred_naive, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- cbind(data.frame(Model = 'Fixed No Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)],cm$overall['Accuracy']))))
all_results
```

Of course our naive approach leads to a perfect specificity but without any sensitivity.


Since computing all these models can be computationally intensive, we first enable parallel computing to speed-up model building.

## Enable parallel computation
```{r parallel on, message=FALSE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

## Logistic Regression

To build a logistic regression model, we use knowledge we gained during explanatory analysis and eliminate highly correlated predictors.

```{r}
mod_glm <- glm(RainTomorrow ~ WindGustSpeed + WindSpeed9am + Humidity3pm + Pressure3pm +
                 MinTemp + MaxTemp + Month + Location, data = train_set, family = binomial)
#summary(mod_glm)
```

All of the selected predictors are highly significant.

```{r}
pred_glm <- predict(mod_glm, type = 'response', newdata = test_set)
```

```{r, message=FALSE, warning=FALSE}
library(ROCR)
pr <- prediction(pred_glm, test_set$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```

```{r, message=FALSE}
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

```{r}
pred_glm <- predict(mod_glm, type = 'response', newdata = test_set)
pred_glm2 <- as.factor(ifelse (pred_glm < 0.3, "No", "Yes"))
cm <- confusionMatrix(data = pred_glm2, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Logistic Regression Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[2,]
```

## Tree

```{r}
library(rpart)
mod_tree <- rpart(RainTomorrow ~ ., data = train_set, method = 'class')
library(rpart.plot)
prp(mod_tree, type = 2, extra = 4)
```

```{r}
pred_tree <- predict(mod_tree, newdata = test_set, type = "class")
cm <- confusionMatrix(pred_tree, test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Simple Tree Model'), as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[3,]
```

```{r, message=FALSE}
pred_tree_prob <- predict(mod_tree, newdata = test_set)
pr <- prediction(pred_tree_prob[,2], test_set$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```

```{r, message=FALSE}
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```


## Random Forest

Correlated variables are less of a problem for random Forests, so we used all predictors instead of selected ones.

```{r random forest, message = FALSE, cache = TRUE}
library(randomForest)
mod_rf <- randomForest(RainTomorrow ~ ., data = train_set, family = binomial)
pred_rf <- predict(mod_rf, type = 'response', newdata = test_set)
cm <- confusionMatrix(data = pred_rf, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Simple Random Forest Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[4,]
```

## Using the caret package

The caret package provides consistency in order to train models from different packages. Available methods are described here : <https://topepo.github.io/caret/train-models-by-tag.html>

Caret also allows to easily use cross validation and to optimize hyperparameters for each type of model.

```{r parallel, message=FALSE}
library(caret)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
```

## Caret - Random Forest

```{r caret rf, cache = TRUE}
Gridrf <-  expand.grid(mtry = c(5, 7, 9))
fit_caret_rf <- train(RainTomorrow ~ ., data = train_set,
                      method = "rf",
                      trControl = fitControl,
                      tuneGrid = Gridrf)
predict_caret_rf <- predict(fit_caret_rf, test_set)
cm <- confusionMatrix(data = predict_caret_rf, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Caret rf Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[5,]
```


## Caret - Bayesian Generalized Linear Model

```{r caret bayesglm, cache = TRUE}
fit_caret_bayesglm <- train(RainTomorrow ~ WindGustSpeed + WindSpeed9am + Humidity3pm + Pressure3pm +
                              MinTemp + MaxTemp + Month + Location, data = train_set,
                            method = "bayesglm",
                            trControl = fitControl)
predict_caret_bayesglm <- predict(fit_caret_bayesglm, test_set)
cm <- confusionMatrix(data = predict_caret_bayesglm, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Bayesian Generalized Linear Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[6,]
```

## Caret XGBoost Model

```{r caret xgboost, cache = TRUE}
Gridxgboost <-  expand.grid(eta = 0.1, 
                            colsample_bytree=c(0.5,0.7),
                            max_depth=c(3,6),
                            nrounds=100,
                            gamma=1,
                            min_child_weight=2,
                            subsample=1)
before <- proc.time()
fit_xgboost <- train(RainTomorrow ~ WindGustSpeed + WindSpeed9am + Humidity3pm + Pressure3pm + MinTemp + MaxTemp + Month + Location,
                     data = train_set,
                     method = "xgbTree",
                     trControl = fitControl,
                     tuneGrid = Gridxgboost)
predict_xgboost <- predict(fit_xgboost, test_set, type = "raw")
proc.time() - before
cm <- confusionMatrix(data = predict_xgboost, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'XGBoost Model'), as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[7,]
```



## Stop parallel computation
```{r parallel off}
stopCluster(cluster)
registerDoSEQ()
```

## Ensemble model

Using several different models and combining them, with a majority vote in our case, can lead to better performance.

```{r ensemble prediction}
predictions <- data.frame(pred_glm2, pred_rf, predict_caret_rf, predict_caret_bayesglm, predict_xgboost)
library(prettyR)
final_prediction <- as.factor(apply(predictions, 1, Mode))
cm <- confusionMatrix(data = final_prediction, reference = test_set$RainTomorrow, positive = "Yes")
all_results <- rbind(all_results, cbind(data.frame(Model = 'Ensemble Majority Vote Model'),
                     as.data.frame(t(c(cm$byClass[c(1,2,7)], cm$overall['Accuracy'])))))
all_results[8,]
```





# Results

```{r}
knitr::kable(all_results)
```

This table sums up the performance of all the models we have tried. Of course many more models are available that could also give good results. the Random Forest models give the best results. Combining all models with a majority vote also gives good results but does not outperform the best model.

# Conclusion

Predicting wether it will rain tomorrow is a binary classification model. The dataset provides many predictors for our prediction, even if there are many missing values. We chose to impute the missing data with sensible values instead of simply focusing on complete cases, which allowed us to keep almost three times as much observations.
We also explored the correlation between those predictors to prepare a feature selection for models which are sensitive to correlated predictors.
We then built a few models, evaluating their performance not only on accuracy but also on sensitivity and specificity and the balance between both with the F1 score. Finally we combined these models with a majority vote.
Even if the prediction is not perfect, it is much better than just guessing or prediction the most common outcome.
"All models are wrong, but some are useful" said George Box. The models we have built hopefully belong to the useful ones.
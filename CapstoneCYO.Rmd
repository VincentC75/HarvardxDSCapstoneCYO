---
title: "HarvardX Data Science Capstone Project"
author: "Vincent"
date: "15 avril 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

# Loading and cleaning data

## Downloading and loading data

The base datafile is downloaded from Kaggle if not already available.

```{r}
# Download base dataset
DataURL <- "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/downloads/weather-dataset-rattle-package.zip/2"
DataFile <- "weather-dataset-rattle-package.zip"
if (!file.exists(DataFile)) {
  download.file(DataURL, destfile = DataFile)
  unzip(DataFile)
}
```

```{r}
rain <- read.csv("weatherAUS.csv")
str(rain)
```

Available columns :

```{r, include = FALSE}
description <- c('The date of observation', 'The common name of the location of the weather station', 'The minimum temperature in degrees celsius', 'The maximum temperature in degrees celsius', 'The amount of rainfall recorded for the day in mm', 'The so-called Class A pan evaporation (mm) in the 24 hours to 9am', 'The number of hours of bright sunshine in the day', 'The direction of the strongest wind gust in the 24 hours to midnight', 'The speed (km/h) of the strongest wind gust in the 24 hours to midnight', 'Direction of the wind at 9am', 'Direction of the wind at 3pm', 'Wind speed (km/hr) averaged over 10 minutes prior to 9am', 'Wind speed (km/hr) averaged over 10 minutes prior to 3pm', 'Humidity (percent) at 9am', 'Humidity (percent) at 3pm', 'Atmospheric pressure (hpa) reduced to mean sea level at 9am', 'Atmospheric pressure (hpa) reduced to mean sea level at 3pm','Fraction of sky obscured by cloud at 9am. This is measured in "oktas", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.', 'Fraction of sky obscured by cloud (in "oktas": eighths) at 3pm. See Cloud9am for a description of the values', 'Temperature (degrees C) at 9am', 'Temperature (degrees C) at 3pm', 'Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0', 'The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the "risk".', 'The target variable. Did it rain tomorrow?')
```

```{r, results='asis'}
knitr::kable(data.frame(column = names(rain), description))
```

The outcome variable, RainTomorrow,  is a factor variables with two levels. This is a binary classification problem

## Cleaning data

### Date

Date is stored as a factor variable. It can be converted to an actual date, however extracting the month or the week should give a better predictor to predict 

```{r}

```

### RISK_MM

This predictor is directly linked to the outcome : "The amount of next day rain in mm. Used to create response variable RainTomorrow.". And in fact there is a perfect corelation between those variables. RainTomorrow is True if RISK_MM > 1.

```{r}
table(RainTomorrow = rain$RainTomorrow, RISK_MM_above_1 = rain$RISK_MM > 1)
```
This predictor must be removed because otherwise it would leak the outcome variable in the dataset and would we not be doing real prediction.

```{r}
rain$RISK_MM <- NULL
```


## Missing values

```{r}
sapply(rain, function(x) sum(is.na(x)))
```


# Exploratory Data Analysis

Let's check the prevalence of the outcome variable
```{r}
table(rain$RainTomorrow)
sum(rain$RainTomorrow == "No") / nrow(rain)
```
We have to take prevalence into account because in 77.5% of cases, the outcome is NO (no rain). So a dummy model always predicting no would be close to 77.5% accurate. We have also to consider sensitivity and specificity. Sensitivity is the ability to predict a positive outcome when the actual outcome is positive. Specificity is the ability not to predict a positive outcome when the actual outcome is not positive.


# Models

## Training and testing set
```{r, warning = FALSE, message=FALSE}
library(caret)
set.seed(1971)
test_index <- createDataPartition(rain$RainTomorrow, times = 1, p = 0.2, list = FALSE)
test_set <- rain[test_index, ]
train_set <- rain[-test_index, ]
```

## Baseline model

A simple and naÃ¯ve model would be to always predict the most frequent outcome. It allows us to have a baseline to evaluate more elaborate models.

```{r}
pred_naive <- rep("No", nrow(test_set))
```

### Overall Accuracy
```{r}
mean(pred_naive == test_set$RainTomorrow)
```
As expected, the overall accuracy is close to the prevalence of the most common outcome. However, if we compute the accuracy by outcome, the weakness of our simple approach is revealed.

```{r, message = FALSE}
library(dplyr)
test_set %>%
  mutate(y_hat = pred_naive) %>%
  group_by(RainTomorrow) %>%
  summarize(accuracy = mean(y_hat == RainTomorrow))
```


### Confusion Matrix

```{r}
pred_naive <- as.factor(pred_naive)
levels(pred_naive) <- levels(test_set$RainTomorrow)
table(predicted = pred_naive, actual = test_set$RainTomorrow)
```

```{r}
confusionMatrix(data = pred_naive, reference = test_set$RainTomorrow)
```

Of course our naive approach leads to a perfect sensitivity but specificity is 0. Balanced accuracy is only 0.5. F1 score is 0.87

```{r}
F_meas(data = pred_naive, reference = test_set$RainTomorrow)
#confusionMatrix(data = pred_naive, reference = test_set$RainTomorrow)$byClass['F1']
```

### ROC Curve

## Logistic Regression

```{r}
#train_glm <- train(RainTomorrow ~ Location, method = "glm", data = train_set)
```

```{r}
#predict_glm <- predict(train_glm, test_set, type = "raw")
#table(predict_glm)
```


## Tree

## Random Forest

## Using the caret package

The caret package provides consistency in order to train models from different packages. Available methods are described here : https://topepo.github.io/caret/train-models-by-tag.html

## Ensemble model

Using several different models and combining them, with a majority vote in our case, can lead to better performance.

# Results

# Conclusion


---
title: "HarvardX Data Science Capstone Project"
author: "Vincent"
date: "May 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![Weather in Australia](australiaweather.jpg)

# Introduction


This project is based on the "Rain in Australia" datasaset available on kaggle: <https://www.kaggle.com/jsphyg/weather-dataset-rattle-package>
The goal is to predict wether or not it will rain tomorrow by training a binary classification model.

# Preparing data

## Downloading and loading data

The base datafile is downloaded from Kaggle if not already available.

```{r}
# Download base dataset
DataURL <- "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/downloads/weather-dataset-rattle-package.zip/2"
DataFile <- "weather-dataset-rattle-package.zip"
if (!file.exists(DataFile)) {
  download.file(DataURL, destfile = DataFile)
  unzip(DataFile)
}
```

```{r}
rain <- read.csv("weatherAUS.csv")
str(rain)
```


## Understanding and cleaning data

### Available columns

```{r, include = FALSE}
description <- c('The date of observation', 'The common name of the location of the weather station', 'The minimum temperature in degrees celsius', 'The maximum temperature in degrees celsius', 'The amount of rainfall recorded for the day in mm', 'The so-called Class A pan evaporation (mm) in the 24 hours to 9am', 'The number of hours of bright sunshine in the day', 'The direction of the strongest wind gust in the 24 hours to midnight', 'The speed (km/h) of the strongest wind gust in the 24 hours to midnight', 'Direction of the wind at 9am', 'Direction of the wind at 3pm', 'Wind speed (km/hr) averaged over 10 minutes prior to 9am', 'Wind speed (km/hr) averaged over 10 minutes prior to 3pm', 'Humidity (percent) at 9am', 'Humidity (percent) at 3pm', 'Atmospheric pressure (hpa) reduced to mean sea level at 9am', 'Atmospheric pressure (hpa) reduced to mean sea level at 3pm','Fraction of sky obscured by cloud at 9am. This is measured in "oktas", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.', 'Fraction of sky obscured by cloud (in "oktas": eighths) at 3pm. See Cloud9am for a description of the values', 'Temperature (degrees C) at 9am', 'Temperature (degrees C) at 3pm', 'Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0', 'The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the "risk".', 'The target variable. Did it rain tomorrow?')
```

```{r, results='asis'}
knitr::kable(data.frame(column = names(rain), description))
```

The outcome variable, RainTomorrow, is a factor variable with two levels. This is a binary classification problem

### Date

Date is stored as a factor variable and can be converted to an actual date.

```{r}
rain$Date <- as.Date(rain$Date)
summary(rain$Date)
```

However extracting the month CAN give can be a better predictor because it will have the same value for the same season over the years.

```{r, message = FALSE}
library(lubridate)
rain$Month <- month(ymd(rain$Date))
```


### RISK_MM and RainTomorrow

This predictor is directly linked to the outcome : "The amount of next day rain in mm. Used to create response variable RainTomorrow.". And in fact there is a perfect corelation between those variables. RainTomorrow is True if RISK_MM > 1.

```{r}
table(RainTomorrow = rain$RainTomorrow, RISK_MM_above_1 = rain$RISK_MM > 1)
```
This predictor must be removed because otherwise it would leak the outcome variable in the dataset.

```{r}
rain$RISK_MM <- NULL
```


### Rainfall and RainToday

RainToday is also a binary representation of the numeric variable Rainfall. Raintoday is true when rainfall > 1mm.

```{r}
table(RainToday = rain$RainToday, rainfall_above_1 = rain$Rainfall > 1)
```
Since those variables are prefectly correlated, we will keep only one of them. Rainfall contains more variability so may have a better predictive value.

```{r}
rain$RainToday <- NULL
```

### Missing values

Some variables contain a high percentage of missing values. 

```{r}
sapply(rain, function(x) round(sum(is.na(x)) / length(x) * 100))
```

for some of them, this percentage is so high that trying to impute those values is useless, therefore we keep only those with up to 10% of missing values and eliminate the others: Evaporation, Sunshine, Cloud9am, Cloud3pm

```{r}
rain$Evaporation <- NULL
rain$Sunshine <- NULL
rain$Cloud9am <- NULL
rain$Cloud3pm <- NULL
```

For the remaining data, we could replace NA with the mean for numerical variable or with the mode for factor variables, but since we are dealing with weather data and we have a date, it's better to assign the last non missing values for the same location.

```{r, message = FALSE}
library(dplyr)
library(tidyr)
rain <- rain %>%
  arrange(Date) %>%
  group_by(Location) %>%
  fill(WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Rainfall, Pressure9am, Pressure3pm, MinTemp, MaxTemp, Temp9am, Temp3pm, WindGustDir, WindDir9am, WindDir3pm) %>%
  fill(WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Rainfall, Pressure9am, Pressure3pm, MinTemp, MaxTemp, Temp9am, Temp3pm, WindGustDir, WindDir9am, WindDir3pm, .direction = "up")
rain <- rain %>% ungroup()
sapply(rain, function(x) sum(is.na(x)))
```

After this imputation, some values are still missing because certain variables where not collected in some locations, and when all values are missing, we cannot impute the previous value.
We can deal with those special cases.
For WindGustDir we impute the value of WindDir9am for the same observation
For WindGustSpeed, we impute the maximum of WindSpeed9am and WindSpeed3pm for the same observation
For Pressure9am and Pressure3pm we have no value in the same observation thta would make sense, so we impute the mean of these variables in the entire dataset.

```{r}
rain$WindGustDir[is.na(rain$WindGustDir)] <- rain$WindDir9am[is.na(rain$WindGustDir)] 
rain$WindGustSpeed[is.na(rain$WindGustSpeed)] <- max(rain$WindSpeed9am[is.na(rain$WindGustSpeed)], rain$WindSpeed3pm[is.na(rain$WindGustSpeed)]) 
rain$Pressure9am[is.na(rain$Pressure9am)] <- mean(rain$Pressure9am[!is.na(rain$Pressure9am)]) 
rain$Pressure3pm[is.na(rain$Pressure3pm)] <- mean(rain$Pressure3pm[!is.na(rain$Pressure3pm)]) 
```

At this stage, we no longer have missing values in the dataset, and we still have 142193 observations. If we had simply ignored all observations with missing values, we would only have 56420 left. This simple imputation allows us to consider almost three times more observations and thus keep more signal in the dataset, hopefully leading to more insight.

# Exploratory Data Analysis

## Prevalence

Let's check the prevalence of the outcome variable
```{r}
prop.table(table(rain$RainTomorrow))
```

We have to take prevalence into account because in 77.5% of cases, the outcome is NO (no rain). So a dummy model always predicting no would be close to 77.5% accurate. We have also to consider sensitivity and specificity. Sensitivity is the ability to predict a positive outcome when the actual outcome is positive. Specificity is the ability not to predict a positive outcome when the actual outcome is not positive.

## Correlation of explanatory variables

The explanotory variables contains different measures of the same quantity. For example we have 4 measures of temperature: min, max, at 9am and at 6pm. As can be expected, there is a relatively strong correlation between these measures.

```{r Temperature Correlation, message = FALSE, warning= FALSE}
library(dplyr)
library(corrplot)
corrplot(cor(select(rain, contains("Temp"))), method = "number")
#pairs(select(rain, contains("Temp")))
```
This is also true for the two measures of pressure.

```{r}
pairs(select(rain, contains("Pressure")))
```

We can outline the highest correlations among the available predictors.

```{r}
numeric <- sapply(rain, is.numeric)
correlations <- cor(rain[,numeric])
diag(correlations) <- 0
high <- apply(abs(correlations) >= 0.7, 2, any)
corrplot(correlations[high, high], method = "number")
```

Some predictors are indeed highly correlated. For instance, the most obvious corelations are between Temp9am and MinTemp, Temp 3pm and MaxTemp. Pressure9am and Pressure3pm also seem to bring the same information whith a near perfect corelation of 0.96.
Later, in models that are sensitive to corelated predictors, we could choose to keep only MinTemp and MaxTemp and only one measure of Pressure. We would keep most of the signal with 3 predictors instead of 6. 

## Humidity

```{r, message=FALSE}
library(ggplot2)
rain %>% 
  ggplot(aes(x=RainTomorrow, y=Humidity9am, colour = RainTomorrow, fill= RainTomorrow)) + geom_violin()
```

```{r, message=FALSE}
rain %>% 
  ggplot(aes(x=RainTomorrow, y=Humidity3pm, colour = RainTomorrow, fill= RainTomorrow)) + geom_violin()
```

We notice that, even if in both cases we are measuring Humidity, there difference is greater in the Humidity3pm measure. It could be interesting to keep only Humidity3pm and eliminate Humidity9am with has a substantial corelation of `round(cor(rain$Humidity9am, rain$Humidity3pm), digits = 2`.


# Models

## Training and testing set
```{r, warning = FALSE, message=FALSE}
library(caret)
set.seed(1971)
test_index <- createDataPartition(rain$RainTomorrow, times = 1, p = 0.2, list = FALSE)
test_set <- rain[test_index, ]
train_set <- rain[-test_index, ]
```

## Baseline model

A simple and naÃ¯ve model would be to always predict the most frequent outcome. It allows us to have a baseline to evaluate more elaborate models.

```{r}
pred_naive <- rep("No", nrow(test_set))
```

### Overall Accuracy
```{r}
mean(pred_naive == test_set$RainTomorrow)
```
As expected, the overall accuracy is close to the prevalence of the most common outcome. However, if we compute the accuracy by outcome, the weakness of our simple approach is revealed. Specificity is perfect but there is no sensitivity.

```{r, message = FALSE}
library(dplyr)
test_set %>%
  mutate(y_hat = pred_naive) %>%
  group_by(RainTomorrow) %>%
  summarize(accuracy = mean(y_hat == RainTomorrow))
```


### Confusion Matrix

```{r}
pred_naive <- as.factor(pred_naive)
levels(pred_naive) <- levels(test_set$RainTomorrow)
table(predicted = pred_naive, actual = test_set$RainTomorrow)
```

```{r}
confusionMatrix(data = pred_naive, reference = test_set$RainTomorrow)
```

Of course our naive approach leads to a perfect sensitivity but specificity is 0. Balanced accuracy is only 0.5. F1 score is 0.87

```{r}
F_meas(data = pred_naive, reference = test_set$RainTomorrow)
#confusionMatrix(data = pred_naive, reference = test_set$RainTomorrow)$byClass['F1']
```

## Logistic Regression

### Feature selection

To build a logistic regression model, we use knowledge we gained during explanatory analysis and eliminate highly correlated predictors.

```{r}
mod_glm <- glm(RainTomorrow ~ WindGustSpeed + WindSpeed9am + Humidity3pm + Pressure3pm + MinTemp + MaxTemp + Month + Location, data = train_set, family = binomial)
#summary(mod_glm)
```

All of the selected predictors are highly significant.

```{r}
pred_glm <- predict(mod_glm, type = 'response', newdata = test_set)
pred_glm2 <- as.factor(ifelse (pred_glm < 0.5, "No", "Yes"))
pred_glm3 <- as.factor(ifelse (pred_glm < 0.3, "No", "Yes"))
confusionMatrix(data = pred_glm2, reference = test_set$RainTomorrow, positive = "Yes")
```
```{r}
#print(paste('Accuracy',round(mean(pred_glm2 == test_set$RainTomorrow), digits = 4)))
```

```{r, message=FALSE, warning=FALSE}
library(ROCR)
pr <- prediction(pred_glm, test_set$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
#plot(prf)
```

```{r, message=FALSE}
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```


## Tree

```{r}
library(rpart)
mod_tree <- rpart(RainTomorrow ~ WindGustSpeed + WindSpeed9am + Humidity3pm + Pressure3pm + MinTemp + MaxTemp + Month + Location, data = train_set, method = 'class')
library(rpart.plot)
prp(mod_tree, type = 2, extra = 4)
```

```{r}
pred_tree <- predict(mod_tree, newdata = test_set, type = "class")
confusionMatrix(pred_tree, test_set$RainTomorrow, positive = "Yes")
```

```{r, message=FALSE}
pred_tree_prob <- predict(mod_tree, newdata = test_set)
pr <- prediction(pred_tree_prob[,2], test_set$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```

```{r, message=FALSE}
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```


## Random Forest

Correlated variables are less of a problem for random Forests, so we used all predictors instead of selected ones.

```{r, message = FALSE, cache = TRUE}
library(randomForest)
mod_rf <- randomForest(RainTomorrow ~ ., data = train_set, family = binomial)
```

```{r}
#importance(mod_rf)
#pred_rf <- predict(mod_rf, type = 'response', newdata = test_set)
#confusionMatrix(data = pred_rf, reference = test_set$RainTomorrow, positive = "Yes")
```



## Using the caret package

The caret package provides consistency in order to train models from different packages. Available methods are described here : https://topepo.github.io/caret/train-models-by-tag.html



```{r}
#train_glm <- train(RainTomorrow ~ Location, method = "glm", data = train_set)
```

```{r}
#predict_glm <- predict(train_glm, test_set, type = "raw")
#table(predict_glm)
```



## Ensemble model

Using several different models and combining them, with a majority vote in our case, can lead to better performance.

# Results

# Improvements

better imputation method ?

# Conclusion

```{r}

```

